{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73409161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_text_file(path, bef, aft, read_empty=False):\n",
    "    \n",
    "    def add_neighbours(*sentcs):\n",
    "        \n",
    "        # unpack list of sentcs passed\n",
    "        lis_sents = list(sentcs)\n",
    "    \n",
    "        joined_tokens = []\n",
    "\n",
    "        for i in range(len(lis_sents)):\n",
    "        \n",
    "            joined_tokens.extend(lis_sents[i].split())\n",
    "        \n",
    "            joined_sent = ' '.join(joined_tokens)\n",
    "    \n",
    "        return joined_sent\n",
    "    \n",
    "    def contextWindow(text, bef, aft):\n",
    "        # 0B, OA --- no change\n",
    "        if (bef == 0) and (aft == 0):\n",
    "            return text\n",
    "        \n",
    "        # 0B, 1A --- 2 conditions  \n",
    "        elif (bef == 0) and (aft == 1):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                # 0B, 1A\n",
    "                if (i >= 0) and (i < len(text)-1):\n",
    "                    contxt_txt.append( add_neighbours(text[i], text[i+1])) \n",
    "                    \n",
    "                # 0B, 0A\n",
    "                elif i == len(text)-1:\n",
    "                    contxt_txt.append(text[i]) \n",
    "                    \n",
    "            return contxt_txt\n",
    "            \n",
    "        #OB, 2A --- 3 conditions\n",
    "        elif (bef == 0) and (aft == 2):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                #OB, 2A\n",
    "                if (i >= 0) and (i <len(text)-2):\n",
    "                    contxt_txt.append( add_neighbours(text[i], text[i+1], text[i+2]))\n",
    "                    \n",
    "                elif i == (len(text)-2):\n",
    "                    contxt_txt.append( add_neighbours(text[i], text[i+1] ))\n",
    "                    \n",
    "                elif i == (len(text)-1):\n",
    "                    contxt_txt.append(text[i])\n",
    "                    \n",
    "            return contxt_txt\n",
    "        \n",
    "        \n",
    "        #1B, 0A --- 2 conditions\n",
    "        elif (bef == 1) and (aft == 0):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                #0B, 0A\n",
    "                if (i == 0):\n",
    "                    contxt_txt.append(text[i])\n",
    "                    \n",
    "                elif (i >0):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))\n",
    "                    \n",
    "            return contxt_txt\n",
    "    \n",
    "        #1B, 1A --- 3 conditions \n",
    "        elif ( bef == 1 ) and ( aft == 1):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i == 0:   # 0B, 1A\n",
    "                    contxt_txt.append(add_neighbours (text[i], text[i+1]))\n",
    "                    \n",
    "                elif (i > 0) and ( i < len(text)-1):  # 1B ,1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i], text[i+1]))\n",
    "                    \n",
    "                elif (i == len(text)-1):  #1B, 0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))\n",
    "            return contxt_txt\n",
    "    \n",
    "        #1B, 2A --- 4 conditions\n",
    "        elif (bef ==1 ) and (aft == 2):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i ==0:\n",
    "                    contxt_txt.append(add_neighbours(text[i], text[i+1], text[i+2]))  #0B, 2A\n",
    "                    \n",
    "                elif (i > 0) and (i < len(text)-2):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i], text[i+1],text[i+2]))     #1B, 2A\n",
    "                    \n",
    "                elif (i == len(text)-2):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i],text[i+1]))               #1B, 1A\n",
    "                    \n",
    "                elif i == (len(text)-1):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))                         #1B,0A\n",
    "            return contxt_txt\n",
    "    \n",
    "        #2B, 0A   -- 3 cases \n",
    "        elif( bef ==2) and ( aft == 0):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i == 0:       \n",
    "                    contxt_txt.append(text[i])         #0B, 0A\n",
    "                elif i == 1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))            #1B, 0A\n",
    "                elif i > 1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i]))  #2B, 0A\n",
    "            return contxt_txt\n",
    "    \n",
    "        #2B, 1A -- 3 cases\n",
    "        elif ( bef == 2 ) and (aft == 1):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i ==0: \n",
    "                    contxt_txt.append(add_neighbours(text[i], text[i+1]) )                     #0B, 1A\n",
    "                    \n",
    "                elif i ==1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i], text[i+1]) )          #1B, 1A\n",
    "                    \n",
    "                elif (i > 1) and (i < len(text)-1):\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i], text[i+1]))  # 2B, 1A\n",
    "                    \n",
    "                elif i == len(text)-1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-2], text[i]))           #2B, 0A\n",
    "                    \n",
    "            return contxt_txt\n",
    "            \n",
    "        #2B, 2A --- 5 conditions\n",
    "        elif (bef == 2) and (aft == 2):        \n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):    \n",
    "        \n",
    "                if i == 0:    # 0B, 2A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2]))  \n",
    "                    \n",
    "                elif i == 1:  # 1B, 2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1] , text[i+2]))\n",
    "                    \n",
    "                elif (i >=2) and (i < len(text)-2):   # 2B , 2A \n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i], text[i+1], text[i+2]))\n",
    "                    \n",
    "                elif (i == len(text)-2):             #2B, 1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i], text[i+1]))\n",
    "                    \n",
    "                elif i == len(text)-1:                #2B, 0A  \n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i]))      \n",
    "                    \n",
    "            return contxt_txt\n",
    "    \n",
    "    text = []\n",
    "    with path.open(mode='r') as in_f:\n",
    "        for line in in_f:\n",
    "            clean_line = line.rstrip()\n",
    "            if clean_line:\n",
    "                text.append(clean_line)\n",
    "            elif read_empty:\n",
    "                text.append([])\n",
    "                \n",
    "    return contextWindow(text, bef, aft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea268dee",
   "metadata": {},
   "source": [
    "# 6 -Types of REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e5f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 NORM_REGEX -normalize\n",
    "\n",
    "import re\n",
    "\n",
    "NORM_REGEX = []\n",
    "NORM_REGEX.append((re.compile(r'\\n(?P<leading_spaces>\\s+)'), '\\n'))\n",
    "NORM_REGEX.append((re.compile(r'^(?P<leading_spaces> +)'), ''))\n",
    "NORM_REGEX.append((re.compile(r'^(?P<leading_newlines>\\n+)'), ''))\n",
    "NORM_REGEX.append((re.compile(r'(?P<trailing_spaces> +)$'), ''))\n",
    "NORM_REGEX.append((re.compile(r'(?P<leading_spaces> +)\\n'), '\\n'))\n",
    "NORM_REGEX.append((re.compile(r'\\n(?P<trailing_spaces> +)'), '\\n'))\n",
    "NORM_REGEX.append((re.compile(r'(?P<multi_spaces>[ \\u200a]{2,})'), ' '))\n",
    "NORM_REGEX.append((re.compile(r'(?P<multi_newline>\\n{2,})'), '\\n'))\n",
    "\n",
    "\n",
    "# 2 regex to split everything there is to split on \".!?\" (but not on \"Word.Word\")\n",
    "SPLIT_REGEX = re.compile(r'\\S.*?(:?(:?(\\.|!|\\?|。|！|？)+(?=\\s+))|(:?(?=\\n+))|(:?(?=\\s*$)))')\n",
    "\n",
    "\n",
    "# 3 splitter assumes new lines as sentence splits. Here new lines are added between costructions that are likely to indicate new sentences. \n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH = []\n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH.append( # add \"save\" cases that capture stuff such as: \"word.word\" \"232.word\" \"word).Next\"\n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]+|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.]) (?P<second_sentence_element>[A-Za-z]+ )'), # regex\n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') # replacement (based on original input)\n",
    "    ) \n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH.append( \n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]+|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.][«”❞’\"]) (?P<second_sentence_element>[A-Za-z]+ )'), \n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') \n",
    "    ) \n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH.append( \n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> formtok) (?P<second_sentence_element>[A-Z][a-z]+ )'), \n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') \n",
    "    ) \n",
    "# TODO: add more cases?\n",
    "\n",
    "REFINED_SPLIT_REGEX_CHANGE_LENGTH = []\n",
    "REFINED_SPLIT_REGEX_CHANGE_LENGTH.append( # add \"save\" cases that capture stuff such as: \"word.word\" \"232.word\" \"word).Next\"\n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]{2,}|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.])(?P<second_sentence_element>[A-HJ-UWYZ][a-z]+ )'), # regex\n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') # replacement (based on original input)\n",
    "    )\n",
    "REFINED_SPLIT_REGEX_CHANGE_LENGTH.append( \n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]{2,}|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.][«”❞’\"])(?P<second_sentence_element>[A-HJ-UWYZ][a-z]+ )'), \n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') \n",
    "    ) \n",
    "\n",
    "REFINED_SPLIT_REGEX = REFINED_SPLIT_REGEX_KEEP_LENGTH + REFINED_SPLIT_REGEX_CHANGE_LENGTH\n",
    "\n",
    "\n",
    "# 4. go in the reverse direction and check for cases where splits are wrong, e.g. when brackets are opened, and only closed on a new line. \n",
    "SUBSENTENCE_REGEX = []\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\([^\\[\\]\\(\\)]*)\\n(?P<close>[^\\[\\]\\(\\)]*\\))'), r'\\g<open> \\g<close>')) # round bracket - no square inbetween\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\[[^\\[\\]\\(\\)]*)\\n(?P<close>[^\\[\\]\\(\\)]*\\])'), r'\\g<open> \\g<close>')) # square bracket - no round inbetween\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\([^\\(\\)]{0,250})\\n(?P<close>[^\\(\\)]{0,250}\\))'), r'\\g<open> \\g<close>')) # round within 250 chars\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\[[^\\[\\]]{0,250})\\n(?P<close>[^\\[\\]]{0,250}\\])'), r'\\g<open> \\g<close>')) # square within 250 chars\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\((?:[^\\(\\)]|\\([^\\(\\)]*\\)){0,250})\\n(?P<close>(?:[^\\(\\)]|\\([^\\(\\)]*\\)){0,250}\\))'), r'\\g<open> \\g<close>')) # round nested\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\[(?:[^\\[\\]]|\\[[^\\[\\]]*\\]){0,250})\\n(?P<close>(?:[^\\[\\]]|\\[[^\\[\\]]*\\]){0,250}\\])'), r'\\g<open> \\g<close>')) # square nested\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>[»“❝‘\"][^»“❝‘\"«”❞’\"]{0,250})\\n(?P<close>[^\\(\\)]{0,250}[«”❞’\"])'), r'\\g<open> \\g<close>')) # quotations within 250 chars\n",
    "\n",
    "\n",
    "\n",
    "# 5. take back even more splits that are not desired\n",
    "\n",
    "ABBREVIATIONS = [r'e\\. ?g\\.\\,?', r'i\\. ?e\\.\\,?', r'i\\. ?v\\.\\,?', r'vs\\.', r'cf\\.', r'c\\. ?f\\.\\,?.', r'Dr\\.', r'Mrs?\\.', r'Ms\\.', r'Ltd\\.\\,?', r'Inc\\.\\,?', r'Corp\\.\\,?', r'wt\\.', r'et ?al\\.', r'sq\\.', r'[Vv]\\.', r'[Vv]er\\.', r'[Ee]xp\\.', r'pp\\.', r'St\\.', r'[Vv]ers\\.', r'spp?\\.', r'ca\\.', r'refs?\\.']\n",
    "\n",
    "\n",
    "RECOMBINE_REGEX = []\n",
    "RECOMBINE_REGEX.append((re.compile(r'[\\.\\,]\\n(?P<normal_word>[a-z]{2}[a-z-]*[ \\.\\:\\,\\;])'), r'. \\g<normal_word>')) # sentence cannot start with a \"normal\" lowercase word.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<abbr>\\b[A-Z]\\.)\\n(?P<name>[a-z]{3,}\\b)'), r'\\g<abbr> \\g<name>')) # names such as: \"S. cerevisiae\"\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[a-z]{2,})\\n(?P<word2>[a-z]{2,}\\s)'), r'\\g<word1> \\g<word2>')) # recombine very \"save\", \"weird\" cases e.g. we performed\\ntests.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[A-Z][a-z]{1,})\\n(?P<word2>[a-z]{2,}\\s)'), r'\\g<word1> \\g<word2>')) # recombine very \"save\", \"weird\" cases e.g. we performed\\ntests.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[a-z]{2,})\\n(?P<word2>http.+?\\s)'), r'\\g<word1> \\g<word2>')) # url..\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[A-Z][a-z]{1,})\\n(?P<word2>http.+?\\s)'), r'\\g<word1> \\g<word2>')) # url..\n",
    "RECOMBINE_REGEX.append( # names such as \"Anton P. Chekhov\", \"A. P. Chekhov\"\n",
    "    (\n",
    "        re.compile(r'(?P<given_names>\\b(?:[A-Z]\\.|[A-Z][a-z]{2,}) [A-Z]\\.)\\n(?P<surname>[A-Z][a-z]+\\b)'),\n",
    "        r'\\g<given_names> \\g<surname>')\n",
    "    )\n",
    "RECOMBINE_REGEX.append((re.compile(r'\\n(?P<word_list>(?:and|or|but|nor|yet|of|in|by|as|on|at|to|via|for|with|that|than|from|into|upon|after|while|during|within|through|between|whereas|whether) )'), r' \\g<word_list>')) # a number of words where there should not be an accidental split.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<t1>\\b[ei]\\.)\\n(?P<t2>[gev]\\.\\,?)'), r'\\g<t1> \\g<t2>')) #specific abbreviations\n",
    "RECOMBINE_REGEX.append( # more abbreviations\n",
    "    (\n",
    "        re.compile(r'(?P<abbr> (' + r'|'.join(ABBREVIATIONS) + r'))\\n'),\n",
    "        r'\\g<abbr> ')\n",
    "    )\n",
    "RECOMBINE_REGEX.append( # still more abbreviations\n",
    "    (re.compile(r'(?P<abbr>\\b([Aa]pprox\\.|[Nn]o\\.|[Ff]igs?\\.|[Tt]bls?\\.|[Ee]qs?\\.))\\n(?P<number>\\d+)'), r'\\g<abbr> \\g<number>'))\n",
    "RECOMBINE_REGEX.append((re.compile(r'(\\.\\s*)\\n(\\s*,)'), r'\\1 \\2')) # commas\n",
    "RECOMBINE_REGEX.append((re.compile(r'\\n(?P<t1>\\(.{0,250}\\))'), r' \\g<t1>')) #specific abbreviations\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<t0>\\d{1,4}\\.?)\\n(?P<t1>\\d{1,4}\\.)'), r'\\g<t0> \\g<t1>')) \n",
    "RECOMBINE_REGEX.append((re.compile(r'\\n(?P<t1>\\[[0-9\\-,\\?]+\\])'), r' \\g<t1>')) \n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<t1>\\d{1,2}\\.)\\n(?P<t2>[12]\\d{3})'), r'\\g<t1> \\g<t2>')) \n",
    "\n",
    "\n",
    "# 6 split enum\n",
    "\n",
    "SPLIT_ENUM_REGEX_KEEP_LENGTH = []\n",
    "SPLIT_ENUM_REGEX_KEEP_LENGTH.append((re.compile(r'(?P<leading>[\\)\\.,;:\\d]) (?P<enum>\\([A-Z0-9]\\)) (?P<next>[A-Z][a-z]+)'), r'\\g<leading>\\n\\g<enum> \\g<next>')) # commas\n",
    "\n",
    "SPLIT_ENUM_REGEX_CHANGE_LENGTH = []\n",
    "SPLIT_ENUM_REGEX_CHANGE_LENGTH.append((re.compile(r'(?P<leading>[\\)\\.,;:\\d])(?P<enum>\\([A-Z0-9]\\)) (?P<next>[A-Z][a-z]+)'), r'\\g<leading>\\n\\g<enum> \\g<next>')) # commas\n",
    "\n",
    "SPLIT_ENUM_REGEX = SPLIT_ENUM_REGEX_KEEP_LENGTH + SPLIT_ENUM_REGEX_CHANGE_LENGTH\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def apply_regex_list(s, regex):\n",
    "\n",
    "    for r, t in regex:\n",
    "        s = r.sub(t, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def _boundary_gen(text, regex):\n",
    "    for match in regex.finditer(text):\n",
    "        yield match.span()\n",
    "        \n",
    "def normalize(s):\n",
    "    \"\"\"Whitespace normalize a string\n",
    "\n",
    "    Args:\n",
    "        s (string): string to normalize\n",
    "\n",
    "    Returns:\n",
    "        string: whitespace normalized string\n",
    "    \"\"\"\n",
    "    replacement_length = []\n",
    "    for r, t in NORM_REGEX:\n",
    "        regex_matches = r.finditer(s)\n",
    "        for match in reversed(list(regex_matches)):\n",
    "            s = s[:match.span(0)[0]] + t + s[match.span(0)[1]:]\n",
    "            replacement_length.append([match.group(0), t, match.span(0)[0], match.span(0)[1]])\n",
    "    return s, replacement_length\n",
    "\n",
    "import re\n",
    "\n",
    "# TODO numbers such as 10,000 and stuff such as R&D\n",
    "TOKENIZATION_REGEX = re.compile(r'((?:10.1371.journal.[a-z]+.[a-z0-9\\.]+)|https?\\:\\/\\/[a-zA-Z0-9\\-\\.]+[\\w\\/\\._\\-\\:~\\?=#%]*[\\w\\/_\\-\\:~\\?=#%]|ftp\\:\\/\\/[a-zA-Z0-9\\-\\.]+[\\w\\/\\._\\-\\:~\\?=#%]*[\\w\\/_\\-\\:~\\?=#%]|www\\.[a-zA-Z0-9\\-\\.]+[\\w\\/\\._\\-\\:~\\?=#%]*|[a-zA-Z0-9\\-\\.]+\\.org\\/[\\w\\/_\\-\\:~\\?=#%]*|[a-zA-Z0-9\\-\\.]+\\.edu\\/[\\w\\/_\\-\\:~\\?=#%]*|[\\.0-9]+[0-9][a-zA-Z]+|v\\.|ver\\.|V\\.|Ver\\.|e\\.g\\.|i\\.e\\.|i\\.v\\.|[0-9]{1,3},[0-9]{3},[0-9]{3}|[0-9]{1,3},[0-9]{3}|\\[[0-9\\-,\\?]+\\]|[0-9\\.]*\\.[0-9]+[a-zA-Z]*|[\\.0-9]+[a-zA-Z]+|[a-qs-uw-zA-QS-UW-Z]+[0-9][a-zA-Z]+|[a-qs-uw-zA-QS-UW-Z][0-9]+[a-zA-Z]?|[a-zA-Z]+&[a-zA-Z]+|[a-zA-Z]+\\.[a-zA-Z]+|[a-zA-Z]+|[0-9]+|[^0-9a-zA-Z\\s])')\n",
    "\n",
    "def tokenize(line):\n",
    "\n",
    "    return [t for t in TOKENIZATION_REGEX.split(line) if t]\n",
    "\n",
    "def sentence_list(s):\n",
    "    \n",
    "    \n",
    "    s_list = s.split('\\n')\n",
    "    \n",
    "    par_token_list = []\n",
    "    for s in s_list:\n",
    "        \n",
    "        s_token_list= tokenize(s)\n",
    "        par_token_list.append(s_token_list)\n",
    "        \n",
    "    return par_token_list\n",
    "\n",
    "# takes txt_o[i], for example txt_o[0] to return list of all sentences in it\n",
    "def sents_in_paragrapgh(txt_o_par):\n",
    "    \n",
    "    # step_1: normalize original text\n",
    "    s = apply_regex_list(txt_o_par, NORM_REGEX)\n",
    "    \n",
    "    #print(s)\n",
    "    \n",
    "    # step_2: offset and split\n",
    "    offsets = [o for o in _boundary_gen(s, SPLIT_REGEX)]\n",
    "    s = '\\n'.join((s[o[0]:o[1]] for o in offsets))\n",
    "    \n",
    "    # setp_3 refined split\n",
    "    s = apply_regex_list(s, REFINED_SPLIT_REGEX)\n",
    "\n",
    "    #  step 4 undo splits  \n",
    "    s = apply_regex_list(s, SUBSENTENCE_REGEX)\n",
    "\n",
    "    #  step 5 undo splits further \n",
    "    s = apply_regex_list(s, RECOMBINE_REGEX)\n",
    "\n",
    "    s = apply_regex_list(s, SPLIT_ENUM_REGEX)\n",
    "    \n",
    "    # tokenize each sentence and return tokenized list of sentences\n",
    "    lis_snts_tkns_par = sentence_list(s)\n",
    "\n",
    "    return lis_snts_tkns_par  \n",
    "\n",
    "\n",
    "def bio_sent_to_paragrapgh(txt_o, txt_bio):\n",
    "    \n",
    "    # stores a list of bio sentences that belong to the same paragrapgh \n",
    "    # key == paragrapgh number\n",
    "    bio_paragrapgh_dict = {}   \n",
    "    \n",
    "    # number of paragrapghs is same as length of original text\n",
    "    num_pars = len(txt_o)\n",
    "    \n",
    "    for i in range(num_pars):\n",
    "        \n",
    "        # list down all sentences in the current i-th paragrapgh, tokenized and normalized\n",
    "        lst_of_sents_in_current_par = sents_in_paragrapgh(txt_o[i])\n",
    "        \n",
    "        # check each sentence in bio list if it is in the current paragrapgh\n",
    "        bio_list_in_par = []\n",
    "        \n",
    "        for bio_snt in txt_bio:\n",
    "            \n",
    "            # if bio_sentence is present in lst_of_sents_in_current_par , append to bio_list_in_par\n",
    "            if bio_snt in lst_of_sents_in_current_par:\n",
    "                bio_list_in_par.append(bio_snt)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        bio_paragrapgh_dict[i] = bio_list_in_par\n",
    "            \n",
    "                \n",
    "    return bio_paragrapgh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d77a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of paragraphs: 5\n",
      "number of sentences in paragrapgh_1: 47\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path_to_file = Path('dax/PMC4507865.data.txt')\n",
    "path_to_label = Path('dax/PMC4507865.labels.txt')\n",
    "path_to_original_file = Path('dax/PMC4507865.txt')\n",
    "\n",
    "txt_bio = _read_text_file(path_to_file,  0, 0)\n",
    "lbl = _read_text_file(path_to_label, 0, 0)\n",
    "txt_o = _read_text_file(path_to_original_file, 0, 0)\n",
    "\n",
    "num_pars = len(txt_o)\n",
    "num_sents_par_1 = len(txt_o[0].split())\n",
    "\n",
    "print(f\"number of paragraphs: {num_pars}\")\n",
    "print(f\"number of sentences in paragrapgh_1: {num_sents_par_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9d807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Study sites and data collection: Study sites and monthly meteorological data collection have previously been described [19].The sites were composed of 13 districts: Banan, Changshou, Fengdu, Fengjie, Fuling, Kaixian, Wanzhou, Shizhu, Wulong, Wushan, Yubei, Yunyang and Zhongxian. These districts are located along the Yangtze River (Fig 1).'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragrapgh_1 = txt_o[0]\n",
    "paragrapgh_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0ce8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_list = sents_in_paragrapgh(paragrapgh_1)\n",
    "#list_of_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd5884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Study sites and data collection : Study sites and monthly meteorological data collection have previously been described [19] .',\n",
       " 'The sites were composed of 13 districts : Banan , Changshou , Fengdu , Fengjie , Fuling , Kaixian , Wanzhou , Shizhu , Wulong , Wushan , Yubei , Yunyang and Zhongxian .',\n",
       " 'These districts are located along the Yangtze River ( Fig 1 ) .',\n",
       " 'Figure data removed from full text .',\n",
       " 'Figure identifier and caption : 10.1371/journal.pone.0133218.g001 Spatial distribution of HFRS occurrence from 1997 to 2008.WS : Wushan ; FJ : Fengjie ; YY : Yunyang ; KX : Kaixian ; WZ : Wanzhou ; SZ : Shizhu ; ZX : Zhongxian ; FD : Fengdu ; WL : Wulong ; FL : Fuling ; CS : Changshou ; YB : Yubei ; BN : Banan .',\n",
       " 'TGD : Three Gorges Dam .',\n",
       " 'HFRS : hemorrhagic fever with renal syndrome .',\n",
       " 'Central region includes the districts of ZX , CS , SZ , FL , FD , and WL .',\n",
       " 'The rest of districts are designated as periphery region .',\n",
       " 'The case of HFRS was selected according to epidemiological data , clinical symptoms and signs .',\n",
       " 'Patient blood samples were collected and sent to local Center for Disease Control and Prevention ( CDC ) institutes for serological and etiological confirmation .',\n",
       " 'All cases were confirmed on antibody tests , pathogen isolation , or evidence of hantavirus RNA sequences in blood or tissues .',\n",
       " 'Patient data used in this study were analyzed and reported anonymously .',\n",
       " 'All participants provided their written informed consent to participate in this study ; and ethical approval for this study was obtained from the Ethical Review Committee of China CDC ( No : 201214 ) .',\n",
       " 'Multiple processes were conducted to control the data quality during HFRS surveillance .',\n",
       " 'First , surveillance system and training guidelines were followed by local CDC employees during data collection and analysis ; any abnormal values were confirmed by either the local CDC or China CDC .',\n",
       " 'Second , reported cases were regularly reviewed to guarantee data integrity .',\n",
       " 'Third , physicians were required to report HFRS cases to the local CDC within 12 hours of the occurrence according to laws and regulations .',\n",
       " 'Rodent data were collected every April and September from 1997 to 2007 in residential area and field areas in the Changshou district .',\n",
       " 'The residential area was selected based on the representative of ecological habitats and the occurrence of HFRS in this district .',\n",
       " 'In residential area , 150 mouse traps were placed daily for a month and effective traps must be higher than 130 traps .',\n",
       " 'One or two traps were placed in a room based on the areas , and five traps were placed in a household for 30 houses .',\n",
       " 'For the field area , forestry and farmland were selected to conduct the investigation of rodent density ; 150 traps were placed per habitat daily for a month .',\n",
       " 'To capture the rodents , mouse traps were set at 5 - meter intervals and baited with peanuts .',\n",
       " 'The captured rodents were taxonomically identified to strain level according to criteria developed by Chen and Qiu [20] .',\n",
       " 'Rodent density was calculated as a proportion ( total number of captured rodents / total number of valid mouse traps ) .',\n",
       " 'An invalid mouse trap was defined as either a missing trap or non - rodent triggered trap .',\n",
       " 'Spatio - temporal analysis of HFRS incidence : Cumulative incidences ( total number of cases / population at the beginning of study ) were calculated for each district to explore the spatial trend of HFRS .',\n",
       " 'The annual / monthly HFRS incidences were calculated for the study area between 1997 and 2008 and curves were plotted to explore the temporal pattern .',\n",
       " 'The number of cases occurring in each month was presented as mean ± standard error of mean to determine the seasonal pattern of HFRS .',\n",
       " 'Poisson regression models were used to explore the association between HFRS incidence and climatic factors .',\n",
       " 'The autocorrelation of incidence was examined and noticeable autocorrelation with lags of 1 , 3 , and 6 months were demonstrated .',\n",
       " 'In these two plots , we found that the lag - 1 , lag - 3 and lag - 6 autocorrelations were 0.573 , 0.458 and 0.512 , respectively , which were significant under the level 0.05 ( the minimum absolute value for autocorrelation coefficient to be significant under 0.05 with the sample size of 140 was calculated to be formtok 0.166 , in which Φ − 1 is the inverse function of cumulative distribution function of standard normal distribution ) ; their partial autocorrelation were 0.573 , 0.279 and 0.220 , which were larger than the other lags up to 12 .',\n",
       " 'Additionally , the AIC values increased when more autocorrelation terms were added to the model .',\n",
       " 'Therefore , the incidence with lags of 1 , 3 and 6 was incorporated into our models as predictors to account for the autocorrelation .',\n",
       " 'Annual gross domestic product ( GDP ) values for Chongqing were included as a predictor to control for the impact of economic improvement on HFRS incidence .',\n",
       " 'A preliminary analysis was conducted using the Poisson regression model with incidence ( lags of 1 , 3 and 6 months ) and GDP values as predictors ; yt denotes the count of HFRS at time t .',\n",
       " 'According to the Poisson regression model , yt followed a Poisson distribution with mean μ t .',\n",
       " 'We denoted the incidence at time t as ratet = μ t / Nt , where Nt denotes the population of a district at time t , the incidence with lag 1 , 3 and 6 months as ratet - 1 , ratet - 3 and ratet - 6 , and GDP value at time t as gdpt .',\n",
       " 'The preliminary analysis model was : Log ( ratet ) = ζ 0 + δ 1 × ratet − 1 + δ 2 × ratet − 3 + δ 3 × ratet − 6 + u2 × gdpt Where δ 1 , δ 2 , δ 3 , and u2 are the corresponding regression coefficients before these predictors .',\n",
       " 'Following the preliminary analysis , the association between HFRS incidence and climatic factors was explored with the incidence ’ s autocorrelation and GDP being adjusted .',\n",
       " 'The multi - collinearity of the different climatic factors with different lags was checked first , then temperature and rainfall data with lags as our candidate predictors were included , based on the multi - collinearity checking .',\n",
       " 'According to the selection process , the final model with the following predictors was constructed : incidence with lag 1 , 3 and 6 months ( ratet - 1 , ratet - 3 and ratet - 6 ) ; GDP with lag 0 months ( gdpt ) ; temperature with lag 0 and 5 months ( denoted as tempt , tempt - 5 ) and rainfall with lag 2 ( denoted as raint - 2 ) .',\n",
       " 'The model is presented here : Log ( ratet ) = ζ 0 + δ 1 × ratet - 1 + δ 2 × ratet - 3 + δ 3 × ratet - 6 + β 21 × tempt + β 22 × tempt − 5 + γ 21 × raint − 2 + u2 × gdpt Autocorrelation plot and residual plot of the residuals were used to examine appropriateness of the models .',\n",
       " 'A zero - inflated negative binomial model was used to explore the relationship between the HFRS incidence and rodent density .',\n",
       " 'Since the number of mice was only collected in April and September in the Changshou district , we aggregated the monthly HFRS incidences into two periods with the same number of months : March - August , and September - February .',\n",
       " 'This aggregation provided more counts of HFRS for each period , which led to more reliable regression analysis .',\n",
       " 'Because the rodent density may impact the disease incidence with lags , rodent density of lags 0 and 1 ( denoted as micet , micet - 1 ) were used as our predictors : where yt denotes the count of HFRS at time t , and ratet = μ t / Nt .',\n",
       " 'The zero - inflated negative binomial model assumed that yt = 0 with probability pt , and yt followed a negative binomial distribution with mean μ t with probability 1 - pt .',\n",
       " 'The following model was formulated : Logit ( pt ) = μ 0 + α 1 × micet + α 2 × micet − 1 Log ( ratet ) = ζ 0 + δ 1 × micet + δ 2 × micet − 1 We used R to estimate the models with the function “ glm ” , which used the iteratively weighted least squares method in order to fit the model and get the parameter estimation .',\n",
       " 'We quantified the significance of parameters with an ( asymptotic ) z - test .',\n",
       " 'Also , we studied model fitting with log - likelihood ( or equivalently , deviance ) and examine basic model assumptions with model diagnostic plots .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c190be94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Study sites and data collection : Study sites and monthly meteorological data collection have previously been described [19] .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_bio[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3701e13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study\n"
     ]
    }
   ],
   "source": [
    "bio_paragrapgh_dict = {}   \n",
    "    \n",
    "# number of paragrapghs is same as length of original text\n",
    "num_pars = len(txt_o)\n",
    "\n",
    "for i in range(num_pars):\n",
    "    \n",
    "    # list of list down all tokens in the current i-th paragrapgh\n",
    "    lsl_tkns_par = sents_in_paragrapgh(txt_o[i])\n",
    "    \n",
    "    #print(lsl_tkns_par)\n",
    "    \n",
    "    # check each sentence in bio list if it is in the current paragrapgh\n",
    "    bio_list_in_par = []\n",
    "    \n",
    "    for bio_snt in txt_bio:\n",
    "        \n",
    "        bio_tkns = bio_snt.split()\n",
    "        \n",
    "        for bio_tkn in bio_tkns:\n",
    "            \n",
    "            print(bio_tkn)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983b1f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Study', ' ', 'sites', ' ', 'and', ' ', 'data', ' ', 'collection', ':', ' ', 'Study', ' ', 'sites', ' ', 'and', ' ', 'monthly', ' ', 'meteorological', ' ', 'data', ' ', 'collection', ' ', 'have', ' ', 'previously', ' ', 'been', ' ', 'described', ' ', '[19]', '.']\n"
     ]
    }
   ],
   "source": [
    "for lst_tkns_par in lsl_tkns_par:\n",
    "    print(lst_tkns_par)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acac3436",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m lst_of_sents_in_current_par \u001b[38;5;241m=\u001b[39m sents_in_paragrapgh(txt_o[i])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# list of tokens in the current paragrapgh\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m paragrapgh_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst_of_sents_in_current_par\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# check each sentence in bio list if it is in the current paragrapgh\u001b[39;00m\n\u001b[1;32m     10\u001b[0m bio_list_in_par \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "for i in range(num_pars):\n",
    "\n",
    "    # list down all sentences in the current i-th paragrapgh, tokenized and normalized\n",
    "    lst_of_sents_in_current_par = sents_in_paragrapgh(txt_o[i])\n",
    "    \n",
    "    # list of tokens in the current paragrapgh\n",
    "    paragrapgh_tokens = ' '.join(lst_of_sents_in_current_par).split()\n",
    "\n",
    "    # check each sentence in bio list if it is in the current paragrapgh\n",
    "    bio_list_in_par = []\n",
    "\n",
    "    for bio_snt in txt_bio:\n",
    "        \n",
    "        bio_sent_tokens = bio_snt.split()\n",
    "        \n",
    "        print(bio_sent_tokens, len(bio_sent_tokens))\n",
    "        break\n",
    "\n",
    "        # if bio_sentence is present in lst_of_sents_in_current_par , append to bio_list_in_par\n",
    "        if bio_snt in lst_of_sents_in_current_par:\n",
    "            \n",
    "            bio_list_in_par.append(bio_snt)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    break\n",
    "\n",
    "    bio_paragrapgh_dict[i] = bio_list_in_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae00da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_paragrapgh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637293d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bio_par_dict = bio_sent_to_paragrapgh(txt_o, txt_bio)\n",
    "bio_par_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bio[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5118d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(len(y)):\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47737381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
