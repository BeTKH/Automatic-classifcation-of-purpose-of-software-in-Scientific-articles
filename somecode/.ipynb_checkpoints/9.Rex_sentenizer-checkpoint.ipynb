{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea268dee",
   "metadata": {},
   "source": [
    "# 6 -Types of REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716fe7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_text_file(path, bef, aft, read_empty=False):\n",
    "    \n",
    "    def add_neighbours(*sentcs):\n",
    "        \n",
    "        # unpack list of sentcs passed\n",
    "        lis_sents = list(sentcs)\n",
    "    \n",
    "        joined_tokens = []\n",
    "\n",
    "        for i in range(len(lis_sents)):\n",
    "        \n",
    "            joined_tokens.extend(lis_sents[i].split())\n",
    "        \n",
    "            joined_sent = ' '.join(joined_tokens)\n",
    "    \n",
    "        return joined_sent\n",
    "\n",
    "    def contextWindow(text, bef, aft):\n",
    "        \n",
    "        #xB, yA   --- where  x,y > len(text)\n",
    "        if (bef >= len(text)-1) or (aft >= len(text)-1):\n",
    "\n",
    "            # reset the context within paragrapgh , returns the whole paragrapgh \n",
    "\n",
    "            bef = len(text)- 1\n",
    "            aft = len(text)- 1\n",
    "\n",
    "            contxt_txt = []        \n",
    "\n",
    "            for i in range(len(text)):  \n",
    "\n",
    "                contxt_txt.append( ' '.join(text))\n",
    "\n",
    "            return contxt_txt\n",
    "            \n",
    "        # 0B, OA --- no change\n",
    "        elif (bef == 0) and (aft == 0):\n",
    "            return text\n",
    "        # 0B, 1A --- 2 conditions  \n",
    "        elif (bef == 0) and (aft == 1):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                # 0B, 1A\n",
    "                if (i >= 0) and (i < len(text)-1):\n",
    "                    contxt_txt.append( add_neighbours(text[i], text[i+1])) \n",
    "\n",
    "                # 0B, 0A\n",
    "                elif (i == len(text)-1):\n",
    "                    contxt_txt.append(text[i]) \n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        #OB, 2A --- 3 conditions\n",
    "        elif (bef == 0) and (aft == 2):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                #OB, 2A\n",
    "                if (i >= 0) and (i <len(text)-2):\n",
    "                    contxt_txt.append( add_neighbours(text[i], text[i+1], text[i+2]))\n",
    "\n",
    "                elif i == (len(text)-2):\n",
    "                    contxt_txt.append( add_neighbours(text[i], text[i+1] ))\n",
    "\n",
    "                elif i == (len(text)-1):\n",
    "                    contxt_txt.append(text[i])\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        #1B, 0A --- 2 conditions\n",
    "        elif (bef == 1) and (aft == 0):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                #0B, 0A\n",
    "                if (i == 0):\n",
    "                    contxt_txt.append(text[i])\n",
    "\n",
    "                elif (i >0):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        #1B, 1A --- 3 conditions \n",
    "        elif ( bef == 1 ) and ( aft == 1):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i == 0:   # 0B, 1A\n",
    "                    contxt_txt.append(add_neighbours (text[i], text[i+1]))\n",
    "\n",
    "                elif (i >= 1) and ( i < len(text)-1):  # 1B ,1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i], text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1):  #1B, 0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))\n",
    "            return contxt_txt\n",
    "\n",
    "        #1B, 2A --- 4 conditions\n",
    "        elif (bef ==1 ) and (aft == 2):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i ==0:\n",
    "                    contxt_txt.append(add_neighbours(text[i], text[i+1], text[i+2]))  #0B, 2A\n",
    "\n",
    "                elif (i > 0) and (i < len(text)-2):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i], text[i+1],text[i+2]))     #1B, 2A\n",
    "\n",
    "                elif (i == len(text)-2):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i],text[i+1]))               #1B, 1A\n",
    "\n",
    "                elif i == (len(text)-1):\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))                         #1B,0A\n",
    "            return contxt_txt\n",
    "\n",
    "        #2B, 0A   -- 3 cases \n",
    "        elif( bef ==2) and ( aft == 0):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i == 0:       \n",
    "                    contxt_txt.append(text[i])         #0B, 0A\n",
    "                elif i == 1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i]))            #1B, 0A\n",
    "                elif i > 1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i]))  #2B, 0A\n",
    "            return contxt_txt\n",
    "\n",
    "        #2B, 1A -- 3 cases\n",
    "        elif ( bef == 2 ) and (aft == 1):\n",
    "            contxt_txt = []\n",
    "            for i in range(len(text)):\n",
    "                if i ==0: \n",
    "                    contxt_txt.append(add_neighbours(text[i], text[i+1]) )                     #0B, 1A\n",
    "\n",
    "                elif i ==1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-1], text[i], text[i+1]) )          #1B, 1A\n",
    "\n",
    "                elif (i > 1) and (i < len(text)-1):\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i], text[i+1]))  # 2B, 1A\n",
    "\n",
    "                elif i == len(text)-1:\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-2], text[i]))           #2B, 0A\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        #2B, 2A --- 5 conditions\n",
    "        elif (bef == 2) and (aft == 2):        \n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):    \n",
    "\n",
    "                #print(f' there are {len(text)} sentences in {text}')\n",
    "\n",
    "                if i == 0:    # 0B, 2A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2]))  \n",
    "\n",
    "                elif i == 1:  # 1B, 2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1] , text[i+2]))\n",
    "\n",
    "                elif (i >=1) and (i < len(text)-2):   # 2B , 2A \n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i], text[i+1], text[i+2]))\n",
    "\n",
    "                elif (i == len(text)-2):             #2B, 1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i], text[i+1]))\n",
    "\n",
    "                elif i == len(text)-1:                #2B, 0A  \n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i]))\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        #3B, 3A --- 7 conditions\n",
    "        elif (bef == 3) and (aft == 3):\n",
    "\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if i == 0:  #0B, 3A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2] , text[i+3] ))\n",
    "\n",
    "                elif i ==1: #1B, 3A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1] , text[i+2] , text[i+3] ))\n",
    "\n",
    "                elif i ==2: #2B, 3A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2] , text[i-1] , text[i] , text[i+1] , text[i+2] ,text[i+3] ))\n",
    "\n",
    "                elif (i >=3) and (i < len(text)-3): #3B, 3A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2] , text[i-1] , text[i] , text[i+1] , text[i+2] , text[i+3] ))\n",
    "\n",
    "                elif (i == len(text)-3): #3B, 2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2] , text[i-1] , text[i] , text[i+1] , text[i+2] ))\n",
    "\n",
    "                elif (i == len(text)-2): #3B, 1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2] , text[i-1] , text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1): #3B, 0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2] , text[i-1] , text[i] ))\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        elif (bef == 0) and (aft == 3):\n",
    "\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if (i >= 0) and (i < len(text)-3): #0B3A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2] , text[i+3] ))\n",
    "\n",
    "                elif (i == len(text)-3):  # 0B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2]))  \n",
    "\n",
    "                elif (i == len(text)-2):  #0B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1):\n",
    "                    contxt_txt.append(text[i])  \n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        elif (bef == 1) and (aft == 3):\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if (i == 0): #0B3A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2] ,  text[i+3] ))\n",
    "\n",
    "                elif (i > 0) and(i < len(text)-3):  # 1B3A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1] , text[i+2], text[i+3] ))  \n",
    "\n",
    "                elif (i == len(text)-3):  #1B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1], text[i+2]))\n",
    "\n",
    "                elif (i == len(text)-2):  #1B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1):  #1B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i]))\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        elif (bef == 2) and (aft == 3):\n",
    "\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if (i == 0): #0B3A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2] ,  text[i+3] ))\n",
    "\n",
    "                elif (i == 1):  # 1B3A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1] , text[i+2], text[i+3] ))  \n",
    "\n",
    "                elif (i > 1) and (i < len(text)-3):  #2B3A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i] , text[i+1], text[i+2],text[i+3]))\n",
    "\n",
    "                elif (i == len(text)-3):  #2B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1] , text[i] , text[i+1], text[i+2]))\n",
    "\n",
    "                elif (i == len(text)-2):  #2B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1] , text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1): #2B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1] , text[i]))\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        elif (bef == 3) and (aft == 2):\n",
    "\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if (i == 0): #0B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1] , text[i+2]))\n",
    "\n",
    "                elif (i == 1):  # 1B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1] , text[i+2]))  \n",
    "\n",
    "                elif (i > 1) and (i < len(text)-3):  #2B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1], text[i] , text[i+1], text[i+2]))\n",
    "\n",
    "                elif (i == len(text)-3):  #3B2A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2], text[i-1] , text[i] , text[i+1], text[i+2]))\n",
    "\n",
    "                elif (i == len(text)-2):  #3B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2], text[i-1] , text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1): #3B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3] , text[i-2], text[i-1] , text[i]))\n",
    "\n",
    "            return contxt_txt \n",
    "\n",
    "        elif (bef == 3) and (aft == 1):\n",
    "\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if (i == 0): #0B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == 1):  # 1B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == 2):  # 2B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1] , text[i] , text[i+1]))  \n",
    "\n",
    "                elif (i > 2) and (i < len(text)-1):  #3B1A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3] , text[i-2], text[i-1], text[i] , text[i+1]))\n",
    "\n",
    "                elif (i == len(text)-1):  #3B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2], text[i-1] , text[i] ))\n",
    "\n",
    "            return contxt_txt\n",
    "\n",
    "        elif (bef == 3) and (aft == 0):\n",
    "            contxt_txt = []        \n",
    "            for i in range(len(text)):\n",
    "\n",
    "                if (i == 0): #0B0A\n",
    "                    contxt_txt.append(text[i])\n",
    "\n",
    "                elif (i == 1):  # 1B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-1] , text[i] ))\n",
    "\n",
    "                elif (i > 1) and ( i < len(text)-1):  # 2B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-2], text[i-1] , text[i]))  \n",
    "\n",
    "                elif (i == len(text)-1):  #3B0A\n",
    "                    contxt_txt.append(add_neighbours(text[i-3], text[i-2], text[i-1] , text[i] ))\n",
    "\n",
    "            return contxt_txt\n",
    "        \n",
    "    txt_with_context = []\n",
    "    with path.open(mode='r') as in_f:\n",
    "\n",
    "        # read new line separated txt\n",
    "        cont = in_f.read()\n",
    "\n",
    "        #create paraggraphs\n",
    "        par_list = cont.split('\\n\\n')\n",
    "\n",
    "\n",
    "        for par_ in par_list:\n",
    "\n",
    "            # list of sentences in the paragrapgh\n",
    "            snt_lst_ = par_.split('\\n')\n",
    "\n",
    "            # remove empty strings\n",
    "            snt_lst_ = list(filter(None, snt_lst_))\n",
    "\n",
    "            # sentence with context\n",
    "            sen_contkx = contextWindow(snt_lst_, bef, aft)\n",
    "            \n",
    "            #print(sen_contkx)\n",
    "\n",
    "            txt_with_context.extend(sen_contkx)\n",
    "            \n",
    "    return txt_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e5f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 NORM_REGEX -normalize\n",
    "\n",
    "import re\n",
    "\n",
    "NORM_REGEX = []\n",
    "NORM_REGEX.append((re.compile(r'\\n(?P<leading_spaces>\\s+)'), '\\n'))\n",
    "NORM_REGEX.append((re.compile(r'^(?P<leading_spaces> +)'), ''))\n",
    "NORM_REGEX.append((re.compile(r'^(?P<leading_newlines>\\n+)'), ''))\n",
    "NORM_REGEX.append((re.compile(r'(?P<trailing_spaces> +)$'), ''))\n",
    "NORM_REGEX.append((re.compile(r'(?P<leading_spaces> +)\\n'), '\\n'))\n",
    "NORM_REGEX.append((re.compile(r'\\n(?P<trailing_spaces> +)'), '\\n'))\n",
    "NORM_REGEX.append((re.compile(r'(?P<multi_spaces>[ \\u200a]{2,})'), ' '))\n",
    "NORM_REGEX.append((re.compile(r'(?P<multi_newline>\\n{2,})'), '\\n'))\n",
    "\n",
    "\n",
    "# 2 regex to split everything there is to split on \".!?\" (but not on \"Word.Word\")\n",
    "SPLIT_REGEX = re.compile(r'\\S.*?(:?(:?(\\.|!|\\?|。|！|？)+(?=\\s+))|(:?(?=\\n+))|(:?(?=\\s*$)))')\n",
    "\n",
    "\n",
    "# 3 splitter assumes new lines as sentence splits. Here new lines are added between costructions that are likely to indicate new sentences. \n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH = []\n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH.append( # add \"save\" cases that capture stuff such as: \"word.word\" \"232.word\" \"word).Next\"\n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]+|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.]) (?P<second_sentence_element>[A-Za-z]+ )'), # regex\n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') # replacement (based on original input)\n",
    "    ) \n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH.append( \n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]+|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.][«”❞’\"]) (?P<second_sentence_element>[A-Za-z]+ )'), \n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') \n",
    "    ) \n",
    "REFINED_SPLIT_REGEX_KEEP_LENGTH.append( \n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> formtok) (?P<second_sentence_element>[A-Z][a-z]+ )'), \n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') \n",
    "    ) \n",
    "# TODO: add more cases?\n",
    "\n",
    "REFINED_SPLIT_REGEX_CHANGE_LENGTH = []\n",
    "REFINED_SPLIT_REGEX_CHANGE_LENGTH.append( # add \"save\" cases that capture stuff such as: \"word.word\" \"232.word\" \"word).Next\"\n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]{2,}|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.])(?P<second_sentence_element>[A-HJ-UWYZ][a-z]+ )'), # regex\n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') # replacement (based on original input)\n",
    "    )\n",
    "REFINED_SPLIT_REGEX_CHANGE_LENGTH.append( \n",
    "    (\n",
    "        re.compile(r'(?P<first_sentence_element> ([A-Za-z\\(\\)\\[\\]]{2,}|[\\d\\(\\)\\[\\]]+) ?[\\?\\!\\.][«”❞’\"])(?P<second_sentence_element>[A-HJ-UWYZ][a-z]+ )'), \n",
    "        r'\\g<first_sentence_element>\\n\\g<second_sentence_element>') \n",
    "    ) \n",
    "\n",
    "REFINED_SPLIT_REGEX = REFINED_SPLIT_REGEX_KEEP_LENGTH + REFINED_SPLIT_REGEX_CHANGE_LENGTH\n",
    "\n",
    "\n",
    "# 4. go in the reverse direction and check for cases where splits are wrong, e.g. when brackets are opened, and only closed on a new line. \n",
    "SUBSENTENCE_REGEX = []\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\([^\\[\\]\\(\\)]*)\\n(?P<close>[^\\[\\]\\(\\)]*\\))'), r'\\g<open> \\g<close>')) # round bracket - no square inbetween\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\[[^\\[\\]\\(\\)]*)\\n(?P<close>[^\\[\\]\\(\\)]*\\])'), r'\\g<open> \\g<close>')) # square bracket - no round inbetween\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\([^\\(\\)]{0,250})\\n(?P<close>[^\\(\\)]{0,250}\\))'), r'\\g<open> \\g<close>')) # round within 250 chars\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\[[^\\[\\]]{0,250})\\n(?P<close>[^\\[\\]]{0,250}\\])'), r'\\g<open> \\g<close>')) # square within 250 chars\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\((?:[^\\(\\)]|\\([^\\(\\)]*\\)){0,250})\\n(?P<close>(?:[^\\(\\)]|\\([^\\(\\)]*\\)){0,250}\\))'), r'\\g<open> \\g<close>')) # round nested\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>\\[(?:[^\\[\\]]|\\[[^\\[\\]]*\\]){0,250})\\n(?P<close>(?:[^\\[\\]]|\\[[^\\[\\]]*\\]){0,250}\\])'), r'\\g<open> \\g<close>')) # square nested\n",
    "SUBSENTENCE_REGEX.append((re.compile(r'(?P<open>[»“❝‘\"][^»“❝‘\"«”❞’\"]{0,250})\\n(?P<close>[^\\(\\)]{0,250}[«”❞’\"])'), r'\\g<open> \\g<close>')) # quotations within 250 chars\n",
    "\n",
    "\n",
    "\n",
    "# 5. take back even more splits that are not desired\n",
    "\n",
    "ABBREVIATIONS = [r'e\\. ?g\\.\\,?', r'i\\. ?e\\.\\,?', r'i\\. ?v\\.\\,?', r'vs\\.', r'cf\\.', r'c\\. ?f\\.\\,?.', r'Dr\\.', r'Mrs?\\.', r'Ms\\.', r'Ltd\\.\\,?', r'Inc\\.\\,?', r'Corp\\.\\,?', r'wt\\.', r'et ?al\\.', r'sq\\.', r'[Vv]\\.', r'[Vv]er\\.', r'[Ee]xp\\.', r'pp\\.', r'St\\.', r'[Vv]ers\\.', r'spp?\\.', r'ca\\.', r'refs?\\.']\n",
    "\n",
    "\n",
    "RECOMBINE_REGEX = []\n",
    "RECOMBINE_REGEX.append((re.compile(r'[\\.\\,]\\n(?P<normal_word>[a-z]{2}[a-z-]*[ \\.\\:\\,\\;])'), r'. \\g<normal_word>')) # sentence cannot start with a \"normal\" lowercase word.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<abbr>\\b[A-Z]\\.)\\n(?P<name>[a-z]{3,}\\b)'), r'\\g<abbr> \\g<name>')) # names such as: \"S. cerevisiae\"\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[a-z]{2,})\\n(?P<word2>[a-z]{2,}\\s)'), r'\\g<word1> \\g<word2>')) # recombine very \"save\", \"weird\" cases e.g. we performed\\ntests.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[A-Z][a-z]{1,})\\n(?P<word2>[a-z]{2,}\\s)'), r'\\g<word1> \\g<word2>')) # recombine very \"save\", \"weird\" cases e.g. we performed\\ntests.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[a-z]{2,})\\n(?P<word2>http.+?\\s)'), r'\\g<word1> \\g<word2>')) # url..\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<word1>\\s[A-Z][a-z]{1,})\\n(?P<word2>http.+?\\s)'), r'\\g<word1> \\g<word2>')) # url..\n",
    "RECOMBINE_REGEX.append( # names such as \"Anton P. Chekhov\", \"A. P. Chekhov\"\n",
    "    (\n",
    "        re.compile(r'(?P<given_names>\\b(?:[A-Z]\\.|[A-Z][a-z]{2,}) [A-Z]\\.)\\n(?P<surname>[A-Z][a-z]+\\b)'),\n",
    "        r'\\g<given_names> \\g<surname>')\n",
    "    )\n",
    "RECOMBINE_REGEX.append((re.compile(r'\\n(?P<word_list>(?:and|or|but|nor|yet|of|in|by|as|on|at|to|via|for|with|that|than|from|into|upon|after|while|during|within|through|between|whereas|whether) )'), r' \\g<word_list>')) # a number of words where there should not be an accidental split.\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<t1>\\b[ei]\\.)\\n(?P<t2>[gev]\\.\\,?)'), r'\\g<t1> \\g<t2>')) #specific abbreviations\n",
    "RECOMBINE_REGEX.append( # more abbreviations\n",
    "    (\n",
    "        re.compile(r'(?P<abbr> (' + r'|'.join(ABBREVIATIONS) + r'))\\n'),\n",
    "        r'\\g<abbr> ')\n",
    "    )\n",
    "RECOMBINE_REGEX.append( # still more abbreviations\n",
    "    (re.compile(r'(?P<abbr>\\b([Aa]pprox\\.|[Nn]o\\.|[Ff]igs?\\.|[Tt]bls?\\.|[Ee]qs?\\.))\\n(?P<number>\\d+)'), r'\\g<abbr> \\g<number>'))\n",
    "RECOMBINE_REGEX.append((re.compile(r'(\\.\\s*)\\n(\\s*,)'), r'\\1 \\2')) # commas\n",
    "RECOMBINE_REGEX.append((re.compile(r'\\n(?P<t1>\\(.{0,250}\\))'), r' \\g<t1>')) #specific abbreviations\n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<t0>\\d{1,4}\\.?)\\n(?P<t1>\\d{1,4}\\.)'), r'\\g<t0> \\g<t1>')) \n",
    "RECOMBINE_REGEX.append((re.compile(r'\\n(?P<t1>\\[[0-9\\-,\\?]+\\])'), r' \\g<t1>')) \n",
    "RECOMBINE_REGEX.append((re.compile(r'(?P<t1>\\d{1,2}\\.)\\n(?P<t2>[12]\\d{3})'), r'\\g<t1> \\g<t2>')) \n",
    "\n",
    "\n",
    "# 6 split enum\n",
    "\n",
    "SPLIT_ENUM_REGEX_KEEP_LENGTH = []\n",
    "SPLIT_ENUM_REGEX_KEEP_LENGTH.append((re.compile(r'(?P<leading>[\\)\\.,;:\\d]) (?P<enum>\\([A-Z0-9]\\)) (?P<next>[A-Z][a-z]+)'), r'\\g<leading>\\n\\g<enum> \\g<next>')) # commas\n",
    "\n",
    "SPLIT_ENUM_REGEX_CHANGE_LENGTH = []\n",
    "SPLIT_ENUM_REGEX_CHANGE_LENGTH.append((re.compile(r'(?P<leading>[\\)\\.,;:\\d])(?P<enum>\\([A-Z0-9]\\)) (?P<next>[A-Z][a-z]+)'), r'\\g<leading>\\n\\g<enum> \\g<next>')) # commas\n",
    "\n",
    "SPLIT_ENUM_REGEX = SPLIT_ENUM_REGEX_KEEP_LENGTH + SPLIT_ENUM_REGEX_CHANGE_LENGTH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "def apply_regex_list(s, regex):\n",
    "\n",
    "    for r, t in regex:\n",
    "        s = r.sub(t, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def _boundary_gen(text, regex):\n",
    "    for match in regex.finditer(text):\n",
    "        yield match.span()\n",
    "        \n",
    "def normalize(s):\n",
    "    \"\"\"Whitespace normalize a string\n",
    "\n",
    "    Args:\n",
    "        s (string): string to normalize\n",
    "\n",
    "    Returns:\n",
    "        string: whitespace normalized string\n",
    "    \"\"\"\n",
    "    replacement_length = []\n",
    "    for r, t in NORM_REGEX:\n",
    "        regex_matches = r.finditer(s)\n",
    "        for match in reversed(list(regex_matches)):\n",
    "            s = s[:match.span(0)[0]] + t + s[match.span(0)[1]:]\n",
    "            replacement_length.append([match.group(0), t, match.span(0)[0], match.span(0)[1]])\n",
    "    return s, replacement_length\n",
    "\n",
    "import re\n",
    "\n",
    "# TODO numbers such as 10,000 and stuff such as R&D\n",
    "TOKENIZATION_REGEX = re.compile(r'((?:10.1371.journal.[a-z]+.[a-z0-9\\.]+)|https?\\:\\/\\/[a-zA-Z0-9\\-\\.]+[\\w\\/\\._\\-\\:~\\?=#%]*[\\w\\/_\\-\\:~\\?=#%]|ftp\\:\\/\\/[a-zA-Z0-9\\-\\.]+[\\w\\/\\._\\-\\:~\\?=#%]*[\\w\\/_\\-\\:~\\?=#%]|www\\.[a-zA-Z0-9\\-\\.]+[\\w\\/\\._\\-\\:~\\?=#%]*|[a-zA-Z0-9\\-\\.]+\\.org\\/[\\w\\/_\\-\\:~\\?=#%]*|[a-zA-Z0-9\\-\\.]+\\.edu\\/[\\w\\/_\\-\\:~\\?=#%]*|[\\.0-9]+[0-9][a-zA-Z]+|v\\.|ver\\.|V\\.|Ver\\.|e\\.g\\.|i\\.e\\.|i\\.v\\.|[0-9]{1,3},[0-9]{3},[0-9]{3}|[0-9]{1,3},[0-9]{3}|\\[[0-9\\-,\\?]+\\]|[0-9\\.]*\\.[0-9]+[a-zA-Z]*|[\\.0-9]+[a-zA-Z]+|[a-qs-uw-zA-QS-UW-Z]+[0-9][a-zA-Z]+|[a-qs-uw-zA-QS-UW-Z][0-9]+[a-zA-Z]?|[a-zA-Z]+&[a-zA-Z]+|[a-zA-Z]+\\.[a-zA-Z]+|[a-zA-Z]+|[0-9]+|[^0-9a-zA-Z\\s])')\n",
    "\n",
    "def tokenize(line):\n",
    "\n",
    "    return [t for t in TOKENIZATION_REGEX.split(line) if t]\n",
    "\n",
    "def sentence_list(s):\n",
    "    \n",
    "    \n",
    "    s_list = s.split('\\n')\n",
    "    \n",
    "    par_token_list = []\n",
    "    for s in s_list:\n",
    "        \n",
    "        s_token_list= tokenize(s)\n",
    "        par_token_list.append(s_token_list)\n",
    "        \n",
    "    return par_token_list\n",
    "\n",
    "# takes txt_o[i], for example txt_o[0] to return list of all sentences in it\n",
    "def sents_in_paragrapgh(txt_o_par):\n",
    "    \n",
    "    # step_1: normalize original text\n",
    "    s = apply_regex_list(txt_o_par, NORM_REGEX)\n",
    "    \n",
    "    #print(s)\n",
    "    \n",
    "    # step_2: offset and split\n",
    "    offsets = [o for o in _boundary_gen(s, SPLIT_REGEX)]\n",
    "    s = '\\n'.join((s[o[0]:o[1]] for o in offsets))\n",
    "    \n",
    "    # setp_3 refined split\n",
    "    s = apply_regex_list(s, REFINED_SPLIT_REGEX)\n",
    "\n",
    "    #  step 4 undo splits  \n",
    "    s = apply_regex_list(s, SUBSENTENCE_REGEX)\n",
    "\n",
    "    #  step 5 undo splits further \n",
    "    s = apply_regex_list(s, RECOMBINE_REGEX)\n",
    "\n",
    "    s = apply_regex_list(s, SPLIT_ENUM_REGEX)\n",
    "    \n",
    "    # tokenize each sentence and return tokenized list of sentences\n",
    "    lis_snts_tkns_par = sentence_list(s)\n",
    "\n",
    "    return lis_snts_tkns_par  \n",
    "\n",
    "\n",
    "def bio_sent_to_paragrapgh(txt_o, txt_bio):\n",
    "    \n",
    "    # stores a list of bio sentences that belong to the same paragrapgh \n",
    "    # key == paragrapgh number\n",
    "    bio_paragrapgh_dict = {}   \n",
    "    \n",
    "    # number of paragrapghs is same as length of original text\n",
    "    num_pars = len(txt_o)\n",
    "    \n",
    "    for i in range(num_pars):\n",
    "        \n",
    "        # list down all sentences in the current i-th paragrapgh, tokenized and normalized\n",
    "        lst_of_sents_in_current_par = sents_in_paragrapgh(txt_o[i])\n",
    "        \n",
    "        # check each sentence in bio list if it is in the current paragrapgh\n",
    "        bio_list_in_par = []\n",
    "        \n",
    "        for bio_snt in txt_bio:\n",
    "            \n",
    "            # if bio_sentence is present in lst_of_sents_in_current_par , append to bio_list_in_par\n",
    "            if bio_snt in lst_of_sents_in_current_par:\n",
    "                bio_list_in_par.extend(bio_snt)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        bio_paragrapgh_dict[i] = bio_list_in_par\n",
    "            \n",
    "                \n",
    "    return bio_paragrapgh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36d77a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of paragraphs: 5\n",
      "number of sentences in paragrapgh_1: 47\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path_to_file = Path('dax/PMC4507865.data.txt')\n",
    "path_to_label = Path('dax/PMC4507865.labels.txt')\n",
    "path_to_original_file = Path('dax/PMC4507865.txt')\n",
    "\n",
    "txt_bio = _read_text_file(path_to_file,  0, 0)\n",
    "lbl = _read_text_file(path_to_label, 0, 0)\n",
    "txt_o = _read_text_file(path_to_original_file, 0, 0)\n",
    "\n",
    "num_pars = len(txt_o)\n",
    "num_sents_par_1 = len(txt_o[0].split())\n",
    "\n",
    "print(f\"number of paragraphs: {num_pars}\")\n",
    "print(f\"number of sentences in paragrapgh_1: {num_sents_par_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab9d807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Study sites and data collection: Study sites and monthly meteorological data collection have previously been described [19].The sites were composed of 13 districts: Banan, Changshou, Fengdu, Fengjie, Fuling, Kaixian, Wanzhou, Shizhu, Wulong, Wushan, Yubei, Yunyang and Zhongxian. These districts are located along the Yangtze River (Fig 1).'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragrapgh_1 = txt_o[0]\n",
    "paragrapgh_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0ce8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_list = sents_in_paragrapgh(paragrapgh_1)\n",
    "#list_of_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3701e13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Study', ' ', 'sites', ' ', 'and', ' ', 'data', ' ', 'collection', ':', ' ', 'Study', ' ', 'sites', ' ', 'and', ' ', 'monthly', ' ', 'meteorological', ' ', 'data', ' ', 'collection', ' ', 'have', ' ', 'previously', ' ', 'been', ' ', 'described', ' ', '[19]', '.'], ['The', ' ', 'sites', ' ', 'were', ' ', 'composed', ' ', 'of', ' ', '13', ' ', 'districts', ':', ' ', 'Banan', ',', ' ', 'Changshou', ',', ' ', 'Fengdu', ',', ' ', 'Fengjie', ',', ' ', 'Fuling', ',', ' ', 'Kaixian', ',', ' ', 'Wanzhou', ',', ' ', 'Shizhu', ',', ' ', 'Wulong', ',', ' ', 'Wushan', ',', ' ', 'Yubei', ',', ' ', 'Yunyang', ' ', 'and', ' ', 'Zhongxian', '.'], ['These', ' ', 'districts', ' ', 'are', ' ', 'located', ' ', 'along', ' ', 'the', ' ', 'Yangtze', ' ', 'River', ' ', '(', 'Fig', ' ', '1', ')', '.']]\n",
      "Study\n"
     ]
    }
   ],
   "source": [
    "bio_paragrapgh_dict = {}   \n",
    "    \n",
    "# number of paragrapghs is same as length of original text\n",
    "num_pars = len(txt_o)\n",
    "\n",
    "for i in range(num_pars):\n",
    "    \n",
    "    # list of list down all tokens in the current i-th paragrapgh\n",
    "    lsl_tkns_par = sents_in_paragrapgh(txt_o[i])\n",
    "    \n",
    "    print(lsl_tkns_par)\n",
    "    \n",
    "    # check each sentence in bio list if it is in the current paragrapgh\n",
    "    bio_list_in_par = []\n",
    "    \n",
    "    for bio_snt in txt_bio:\n",
    "        \n",
    "        bio_tkns = bio_snt.split()\n",
    "        \n",
    "        for bio_tkn in bio_tkns:\n",
    "            \n",
    "            print(bio_tkn)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break\n",
    "    \n",
    "    break\n",
    "    bio_paragrapgh_dict[i] = bio_list_in_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd3ad419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
