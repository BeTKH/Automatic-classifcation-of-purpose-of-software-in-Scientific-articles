\chapter{Conclusion and Future work}
\label{ch:chapter07}
 
%
% Section:7 - Intro
%

\section{Summary of Results }
\label{sec:chapter06:summary}

Evaluation of \ac{BERT} models used in this thesis, \ac{Sci-BERT} and \ac{Bio-BERT}, revealed important observations. Firstly, the analysis of Sci-BERT \emph{software-purpose} classification indicates that, the composition of training data set has significant impact on the software purpose classification. Overall, when Sci-BERT classifier model is trained by including parts of the SoMeSci dataset, that lacks software purpose annotations, software purpose classifier's performance is deteriorated. However, classification performance for software purpose is increased when the model is trained without parts of SoMeSci data set that lacks software-purpose annotations. This result clearly indicates, including unlabeled dataset in the training data degrades software-purpose classification performance.  \\

Secondly, it has been observed that various levels of context can affect classifier model's performance. Training of a classifier model based on neighboring sentences, in a scientific articles of SoMeSci dataset, has been seen to improve software purpose classification performance. Typically, a context information with two sentences at the left-side (2,0) of a given sentence appears to give more important clue for the \ac{Sci-BERT} \emph{software-purpose} classifier. However, unbounded context information such as the whole paragraph or a context not limited within a paragraph did not provide better classification performance compared to context information limited to only two sentences from the left-side(2,0) as shown on the figure 6.4. \\


Thirdly, evaluation of the Sci-BERT model's performance by removing the intermediate modules of \emph{software-type} and \emph{mention-type} classifiers from the 4-cascade classifier, produced a slightly better performance for \emph{software purpose} classification.  This indicates that flawed classifications, from the middle classifier modules of the 4-cascade, potentially impact \emph{software-purpose} classifier's performance which lies at the end of cascade of classifier modules. \\

Further more, it was observed that training other variants of BERT model such as Bio-BERT-base and Bio-BERT-large model results in a different software-purpose classification performance. The Bio-BERT-large model has better performance compared with Sci-BERT but it is too slow for training. On the other hand Bio-BERT-small trains fast but its software purpose classification performance is lower than Sci-BERT. \\


Generally, it is was also observed that \emph{software purpose} classification performance for software purposes such as \emph{analysis} has much more classification performance compared to other software usage purposes like \emph{stimulation or simulation} which has smaller number of class labels.   


\section{Conclusion}
\label{sec:chapter07:Conclusion}


\section{Future Work}
\label{sec:chapter07:futurework}


