\chapter{Model Training and Optimization}
\label{ch:chapter06}
 
%
% Section: 6 - Intro
%

\ac{BERT}, specifically \ac{Sci-BERT}, multi-class classifier model has been trained and tested using \ac{SoMeSci} data set. The model has been trained in various training scenarios to determine conditions that leads to improved performance of classifier. The training scenarios considered, the effect of context information  and the impact of including or excluding parts of \ac{SoMeSci} data set on the classifier’s performance.\\
 
Furthermore, software purpose classifier’s performance has also been investigated by removing mention-type and software-type classifiers from cascades of classifier modules.  In addition, the impact of using another variant of BERT model, \ac{Bio-BERT}, on classifier’s performance  has also been investigated. \\

Finally, results from all training scenarios have been discussed and best performing model has been selected based on the results of investigation.

\section{Model training with inclusion/exclusion part of data}
\label{sec:chapter06:exclusion}

As described in the table 4.1 before, the \ac{SoMeSci} dataset is composed of 4 different sets of articles: \emph{PLoS-methods, PubMed-full text,  PLoS-sentences} and \emph{creation-sentences}. Since only articles in the  \emph{PLoS-methods} and \emph{PubMed-full text} are annotated with software usage purpose labels, it was desired to evaluate weather including PLoS and creation would result in improved performance of the software usage purpose classifier.  \\

The results of evaluation indicates that including \emph{ PLoS-sentences} and \emph{creation-sentences} in the training dataset, definitely improved the overall performance for classification of software. Figure 6.1 below shows total F-score for software classification over test and development data sets is increased with the inclusion of \emph{PLoS/Creation} sentences in the data set. \\

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.86\textwidth]{4.graphics/figures/ch_6/1.with_sent_vs_without/HD/TotalFscoresoftware}
	\caption{Software classification (Total) F1-score for devel (left) and test(right) shows improvement when \emph{Creation/PLoS} sentences are included in the training dataset.}
	\label{fig:chapter06:with}
\end{figure}


In contrast, the overall performance of the software purpose classifier is diminished when trained with \emph{ PLoS-sentences} and \emph{creation-sentences} in addition to \emph{ PLoS-methods}, \emph{PubMed-full text}. Figure 6.2 below depicts, Fscore for software purpose classifier is deteriorated when trained along with \emph{Creation/PLoS} sentences.  \\

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.86\textwidth]{4.graphics/figures/ch_6/1.with_sent_vs_without/HD/Total_Fscore_software_purpose}
	\caption{Software purpose classification (Total) F1-score degrades for devel. set (left) and test set (right) when \emph{Creation/PLoS} sentences are included in the training dataset.}
	\label{fig:chapter06:with}
\end{figure}


\begin{table}[ht]
	\centering
	\caption{Evaluation of software purpose classifier's performance with(+) and without(-) \emph{Creation/PLoS}sentences in the training data set.}
	\begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}  l  l l  l l l } %l left align , c- center
		\hline
		Software Purpose & Metric & Dev+        & Dev-     & Test+  &Test- \\
		\hline 
		Analysis         & F     & 0.64        & 0.71     &0.65     &0.69   \\
		                 & P     & 0.61        & 0.71     &0.60     &0.66   \\
		                 & R     & 0.68        & 0.71     &0.72     &0.73   \\
		\hline
		Data-Collection  & F     & 0.26        &  0.32    & 0.26    & 0.34  \\
					     & P     & 0.25        &  0.33    & 0.22    & 0.30  \\
						 & R     & 0.28        &  0.31    & 0.30    & 0.31  \\		
		
		\hline
		Pre-Processing   & F     & 0.41        &  0.45    & 0.49    & 0.57  \\
						 & P     & 0.36        &  0.37    & 0.45    & 0.49  \\
						 & R     & 0.48        &  0.56    & 0.58    & 0.67  \\
		\hline
		Modeling         & F     & 0.25        &  0.54    & 0.30    & 0.44  \\
					     & P     & 0.27        &  0.54    & 0.28    & 0.42  \\
						 & R     & 0.24        &  0.55    & 0.30    & 0.46  \\
	
		\hline
		Programming      & F     & 0.27        &  0.42    & 0.23    & 0.42  \\
						 & P     & 0.28        &  0.34    & 0.24    & 0.36  \\
						 & R     & 0.27        &  0.55    & 0.23    & 0.51  \\
		
		\hline
		Simulation      & F     & 0.00        &  0.38    & 0.00    & 0.00 \\
						& P     & 0.00        &  0.88    & 0.00    & 0.00  \\
						& R     & 0.00        &  0.25    & 0.00    & 0.00  \\
		
		\hline
		Stimulation     & F     & 0.46        &  0.59    & 0.24    & 0.26 \\
						& P     & 0.40        &  0.76    & 0.21    & 0.24  \\
						& R     & 0.48        &  0.45    & 0.28    & 0.22  \\
		
		\hline
		Visualization   & F     & 0.48        &  0.59    & 0.52    & 0.58  \\
						& P     & 0.46        &  0.59    & 0.46    & 0.54  \\
						& R     & 0.51        &  0.61    & 0.56    & 0.62  \\
		\hline
		Total (software Purpose)	& F     & 0.27        &  0.58*    & 0.30    & 0.54*  \\
								& P     & 0.26        &  0.62*    & 0.26    & 0.56*  \\
								& R     & 0.31        &  0.55*    & 0.32    & 0.65*  \\
		\hline
		Total (software) 	& F     &  0.86*       & 0.84     & 0.83    & 0.83  \\
						& P     &  0.83*       & 0.82     & 0.77    & 0.79*  \\
						& R     &  0.90*       & 0.86     & 0.90*    & 0.87  \\
		\hline
	\end{tabular*}
\end{table}%

Software purpose classifier's performance when trained with \emph{creation/PLoS} sentences versus  without is summarized on the table 6.1. \\

Overall, as indicated in table 6.1, it is observed that including a data set that lacks software purpose annotation harms classifier model’s performance. Since the main goal of this project is to maximize software usage purpose classifer’s performance, for subsequent steps of analysis, the software usage purpose classifier has been trained only with datasets of \emph{PLoS-methods} and \emph{PubMed-full text} by excluding \emph{Creation/PLoS} sentences. 

\section{The impact of context on classifier’s performance }
\label{sec:chapter06:context}

Scientific papers like any other well written documents, have a sequential structure that form abstraction at various levels such as sentence, paragraphs, sections, chapters, etc. These levels of abstractions often determine the meaning of words because each level of abstraction or context conveys a valuable information \citep{ghosh2016contextual}. \\

Likewise, contextual information helps to determine the correct purpose of use of a software. Therefore for each sentence in a text of scientific articles of training data set, various range of neighboring sentences have been incorporated to provide a contextual information. Typically 2 adjacent sentences before a sentence, after a sentence, and before \& after a sentence have been considered. Further more, a context as broad as the whole paragraph has also been considered. \\

Excerpt of the python code for reading neighboring sentences for context has been listed on the \emph{appendix E}. The complete code has been listed on git-hub inside a methods that reads a file  \footnote{\url{https://github.com/BeTKH/SoMeNLP/blob/contxt2Sentcs_wo/somenlp/NER/data_handler.py}}. 

\subsection{Left Context vs Right Context of Sentence}
\label{sec:chapter06:leftvsright}

Left context refers to sentences prior to a given sentence with in  a paragraph and right context refers to sentences that lie right after a given sentence. \\

Classifier’s performance evaluation indicate that there is a slight improvement in model’s performance for right context as compared to the left context. However, when 2 sentences to left as well as right are taken into account for context, there is no significant gain on the model’s performance for both software and purpose of usage. \\

However, due to the nature of randomness in the training of the classifier model, each instance of training round tends to give slightly different result. Because of this, both left and right context of two sentences have been considered for subsequent steps of analysis of the model despite the apparent significance of right context over the left. \\

\begin{figure}[h]
	
	\myfloatalign
	
	\subfloat{
		\label{fig:chapter03:subfloat:grafik1}
		\includegraphics[width=.90\linewidth]{4.graphics/figures/ch_6/2.left_context_vs_right/HD/1}
	} \\
	\subfloat{
		\label{fig:chapter03:subfloat:grafik4}
		\includegraphics[width=.90\linewidth]{4.graphics/figures/ch_6/2.left_context_vs_right/HD/2}
	}\\
	\caption{The effect of context information on software purpose classifier's performance: right context gives improved performance over left context as well as left and right context.}
\end{figure}

Theoretically consideration of broader context, more than two adjacent sentences, is supposed to improve the classifier’s performance. However, the result of the analysis reveals that a context as big as the whole paragraph did not contribute to the model’s classification performance. Rather, a smaller window of context such as two sentences left and right appears to have a better performance. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.90\textwidth]{4.graphics/figures/ch_6/3.paragraph_context_vs_22/HD/1}
	\caption{Consideration of broader context leads to loss of classifer's performance.}
	\label{fig:chapter06:with}
\end{figure}


\subsection{Context Outside a Paragraph}
\label{sec:chapter06:contxtOutside}


The other scenario considered was evaluation of model’s performance when context is not limited to a paragraph. The results indicate that, classifier’s performance degraded when a context is not limited within a paragraph. This agrees with the fact that each paragraph of a scientific publication conveys a specific information and a contextual information outside a paragraph might not be useful for the classifier. The classifier model which considers context outside a paragraph has been listed on \emph{“contxt\_out\_parg”} git-hub branch of SoMeNLP project\footnote{\url{https://github.com/BeTKH/SoMeNLP/tree/contxt_out_parg}}. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.90\textwidth]{4.graphics/figures/ch_6/4.context_outside_paragraph_vs_inside/HD/1}
	\caption{The impact of context outside a paragraph vs inside.}
	\label{fig:chapter06:with}
\end{figure}

\section{Classification with 2-layers}
\label{sec:chapter06:2lc}

Overall the 4-layred \ac{Bi-LSTM-CRF} multi-class classifier’s performance has been poor regardless of consideration of various factors discussed above. Because of this, the model has been simplified into a two layered Bi-LSTM-CRF model by removing software-type and mention-type classifiers. \\

Truncating the model into two layers helped to evaluate the software purpose classifiers performance exclusively by removing the effects of intermediate layers of mention-type and software-tape classifiers. \\

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.90\textwidth]{4.graphics/figures/ch_5/2LC}
	\caption{Two layered classifier fully connected model.}
	\label{fig:chapter06:with}
\end{figure}

Evaluation of the 2-layered classifier model reveals that overall software purpose classification performance has shown small improvement compared to the original 4-layered \ac{Bi-LSTM-CRF} model, where as the software classifier’s performance does not indicate any performance improvement.  

\begin{figure}[h]
	
	\myfloatalign
	
	\subfloat{
		\label{fig:chapter03:subfloat:grafik1}
		\includegraphics[width=.90\linewidth]{4.graphics/figures/ch_6/5.2layerClassifier/HD/1}
	} \\
	\subfloat{
		\label{fig:chapter03:subfloat:grafik4}
		\includegraphics[width=.90\linewidth]{4.graphics/figures/ch_6/5.2layerClassifier/HD/2}
	}\\
	\caption{The Truncated 2-layer \ac{Bi-LSTM-CRF} classifier performed slightly better compared to the 4 layered model.}
\end{figure}


\section{Model Training Bio-BERT vs Sci-BERT}
\label{sec:chapter06:biosci}

Even though both \ac{Bio-BERT} and \ac{Sci-BERT} give a contextualized representation of a word in a sentence, representations of a word would differ due to the inherent difference of corpora used for pre-trained models of Bio-BERT and Sci-BERT \citep{beltagy2019scibert,li2019fine}. For this reason, classifier’s performance has been evaluated for both Sci-BERT and Bio-BERT models. \\

The results of evaluation indicate that the classifier models performed slightly better when using Bio-BERT large\footnote{\url{https://huggingface.co/dmis-lab/biobert-large-cased-v1.1}} embedding compared to Sci-BERT\footnote{\url{https://huggingface.co/allenai/scibert_scivocab_cased}}. However, the model performed better with Sci-BERT compared to the Bio-BERT small embedding\footnote{\url{https://huggingface.co/dmis-lab/biobert-base-cased-v1.2}}. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.90\textwidth]{4.graphics/figures/ch_6/6.BIoBERT_vs_SCIBERT_2LAYER_Classifier/HD/1}
	\caption{Two layered classifier fully connected model.}
	\label{fig:chapter06:with}
\end{figure}

\section{Summary of Training Results }
\label{sec:chapter06:summary}

Performance evaluation of the Bi-LSTM-CRF multi-class classifier model, in various training scenarios, reveals some interesting insights. \\

Firstly, the analysis of the model performance results indicate that, the composition of the data set has significant impact on the models performance. As stated before, when the classifier model is fed with data set that lacks software purpose annotations in creation and plos sentences, the models performance decreased. \\

Secondly, it has been observed that various levels of context can affect models performance. As presented earlier, when neighboring sentences in a scientific articles of SoMeSci data set is considered it has been clearly observed that models performance has shown improvement. However, the analysis also reveals that unbounded context information such as a context outside a given paragraph is not useful to the model’s performance, rather it will degrade the models performance. Further more, it has been observed that a context of sentences on the left and right might not be equally important. The analysis results indicated that a context of sentences from the right were observed to be more useful to the models performance than the left. \\

Thirdly, evaluation of the classifier’s performance by truncating the intermediate layers of classifiers indicated a marginal improvement of software purpose classifier which lies at the end of the 4-layered chain of classifiers but there was no tangible performance gain was observed for the software classifier. This might indicate that, probably there flawed classifications of intermediate layers might have an impact on the software purpose classifier. \\

Further, it was also observed that the use of various types of embeddings like Bio-BERT and Sci-BERT impacted the classifier’s performance. The use of larger variants of embedding models was observed to improve classifiers performance. \\

Ultimately, it is reasonable to conclude that the study of all training scenarios and consideration of factors does not show a very large scale performance gain or loss. This might be because a constraint on the amount of labeled data set and unavailability of larger size of training data set. 



\subsection{Summary of Software Purpose Classifier performance}
\label{sec:chapter06:summary_softPurpose}

\subsection{Summary of Software Classifier performance}
\label{sec:chapter06:summary_soft}



















